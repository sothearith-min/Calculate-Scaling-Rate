{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import warnings\n",
    "from scipy.stats import linregress\n",
    "import geopandas as gpd\n",
    "\n",
    "def calculate_dewpoint(temp, humidity):\n",
    "    A = 17.27\n",
    "    B = 237.7\n",
    "    alpha = ((A * temp) / (B + temp)) + np.log(humidity/100.0)\n",
    "    return (B * alpha) / (A - alpha)\n",
    "\n",
    "def process_data(interp_data, region_data):\n",
    "    for year, df in interp_data.items():\n",
    "        df = pd.merge(df, region_data, on=[\"lat\", \"lon\"], how=\"inner\")\n",
    "        last_column = df.pop(df.columns[-1])\n",
    "        df.insert(0, last_column.name, last_column)\n",
    "        df = df.drop([\"lat\", \"lon\"], axis=1)\n",
    "        interp_data[year] = df\n",
    "\n",
    "def calculate_bins(series):\n",
    "    series_range = series.max() - series.min()\n",
    "    return int(series_range / 1)\n",
    "\n",
    "\n",
    "def region_scaling(pr_dir, tas_dir, hur_dir, start_year, stop_year, region,  method = None):\n",
    "    years_list = [str(year) for year in range(start_year, stop_year+1)]\n",
    "    interp_pr, interp_tas, interp_hur, interp_tdew = {}, {}, {}, {}\n",
    "\n",
    "    for year in years_list: \n",
    "        file_name = f\"{year}.csv\"\n",
    "        # pr\n",
    "        file_pr = pd.read_csv(pr_dir + \"\\\\\" + file_name)\n",
    "        file_pr = file_pr[file_pr.lat > -60]\n",
    "        # tas\n",
    "        file_tas = pd.read_csv(tas_dir+ \"\\\\\" + file_name)\n",
    "        file_tas = file_tas[file_tas.lat > -60]\n",
    "        # hur\n",
    "        file_hur = pd.read_csv(hur_dir+ \"\\\\\" + file_name)\n",
    "        file_hur = file_hur[file_hur.lat > -60]\n",
    "        interp_pr[year], interp_tas[year], interp_hur[year] = file_pr, file_tas, file_hur \n",
    "    \n",
    "    # Calculate Dewpoint temperature \n",
    "\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "\n",
    "    interp_tdew = {}\n",
    "\n",
    "    for key in interp_tas.keys():\n",
    "        tas = interp_tas[key]\n",
    "        hur = interp_hur[key]\n",
    "        tdew = pd.DataFrame()\n",
    "        tdew[[\"lat\", \"lon\"]] = tas[[\"lat\", \"lon\"]]\n",
    "        for day in tas.columns[2:]:\n",
    "            tdew[day] = calculate_dewpoint(tas[day], hur[day])\n",
    "        interp_tdew[key] = tdew \n",
    "\n",
    "    if region == \"ar6\":\n",
    "        grouped = pd.read_csv(\"D:\\Min\\Review GCM\\Region and Location\\Grouping_Region_AR6.csv\")[[\"lat\", \"lon\", \"code\"]]\n",
    "    elif region == \"srex\":\n",
    "        grouped = pd.read_csv(\"D:\\Min\\Review GCM\\Region and Location\\Grouping_Region_SREX.csv\")[[\"lat\", \"lon\", \"code\"]]\n",
    "\n",
    "    # Match with Pr and Tdew dict \n",
    "        \n",
    "    process_data(interp_pr, grouped)\n",
    "    process_data(interp_tdew, grouped)\n",
    "\n",
    "    # Combine tdew and Pr \n",
    "\n",
    "    pr = pd.concat([df.set_index([\"code\"]) for df in list(interp_pr.values())], axis=1).reset_index()\n",
    "    pr_long = pd.melt(pr, id_vars=['code'], var_name='date', value_name='pr')\n",
    "    tdew  = pd.concat([df.set_index(['code']) for df in list(interp_tdew.values())], axis=1).reset_index()\n",
    "    tdew_long = pd.melt(tdew, id_vars=['code'], var_name='date', value_name='tdew')\n",
    "    pr_long[\"tdew\"] = tdew_long['tdew'].values\n",
    "\n",
    "    # get Wet-day \n",
    "\n",
    "    pr_long = pr_long[pr_long.pr > 0.1]\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    if method == \"quantile_regression\" or method is None: \n",
    "\n",
    "        qr =  pr_long.groupby('code').apply(lambda group: sm.QuantReg(np.log(group['pr']), sm.add_constant(group[['tdew']])).fit(q=0.99))\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for group_key, group_result in qr.items():\n",
    "            code = group_key\n",
    "            slope_coefficient = group_result.params[\"tdew\"]\n",
    "            df = pd.DataFrame({'code': [code], 'slope_coefficient': slope_coefficient})\n",
    "            dfs.append(df)\n",
    "        \n",
    "        result_qr = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        result_qr[\"Scaling\"] =  100*(np.e**result_qr[\"slope_coefficient\"] - 1)\n",
    "        result_qr = result_qr[[\"code\", \"Scaling\"]]\n",
    "\n",
    "        # Add to result dict \n",
    "        result[\"quantile_regression\"] = result_qr\n",
    "\n",
    "    if method == \"binning_data_point\" or method is None:\n",
    "        df = pr_long.copy()\n",
    "        n_bins = 30\n",
    "        df[\"bin\"] = df.groupby('code')['tdew'].transform(lambda x: pd.qcut(x, q=n_bins, labels=False, duplicates='drop'))\n",
    "        dfs = df.drop(['date'], axis = 1)\n",
    "\n",
    "        bm = dfs.groupby(['code', 'bin']).agg({'pr': lambda x: x.quantile(0.99), 'tdew': 'mean'}).reset_index()\n",
    "        bm['log_p_99'] = bm['pr'].apply(lambda x: 0 if x <= 0 else 1 if x == 1 else np.log(x))\n",
    "        bm.columns = ['code', 'bin', 'p_99_pr', 'mean_tdew', 'log_p99']\n",
    "\n",
    "        slopes = []\n",
    "\n",
    "        for code, group in bm.groupby('code'):\n",
    "\n",
    "            slope, _, _, _, _ = linregress(group['mean_tdew'], group['log_p99'])\n",
    "\n",
    "            slopes.append({'code': code, 'slope': slope})\n",
    "\n",
    "        result_bm = pd.DataFrame(slopes)\n",
    "\n",
    "        result_bm[\"Scaling\"] = 100*(np.e**result_bm[\"slope\"] - 1)\n",
    "        result_bm = result_bm[[\"code\", \"Scaling\"]]\n",
    "\n",
    "        # Add to result dict \n",
    "        result[\"binning_equal_data\"] = result_bm\n",
    "        \n",
    "\n",
    "    if method == \"binning_equal_width\" or method is None:\n",
    "        df = pr_long.copy()\n",
    "        df[\"bin\"] = df.groupby('code')['tdew'].transform(lambda x: pd.cut(x, bins=calculate_bins(x), labels=False, include_lowest=True))\n",
    "\n",
    "        dfs = df.drop(['date'], axis = 1)\n",
    "\n",
    "        bm_width =  dfs.groupby(['code', 'bin']).agg({'pr': lambda x: x.quantile(0.99), 'tdew': 'mean'}).reset_index()\n",
    "        bm_width['log_p_99'] = bm_width['pr'].apply(lambda x: 0 if x <= 0 else 1 if x == 1 else np.log(x))\n",
    "        bm_width.columns = ['code', 'bin', 'p_99_pr', 'mean_tdew', 'log_p99']\n",
    "\n",
    "        slopes = []\n",
    "\n",
    "        for code, group in bm_width.groupby('code'):\n",
    "\n",
    "            slope, _, _, _, _ = linregress(group['mean_tdew'], group['log_p99'])\n",
    "\n",
    "            slopes.append({'code': code, 'slope': slope})\n",
    "\n",
    "        result_bm_width = pd.DataFrame(slopes)\n",
    "\n",
    "        result_bm_width[\"Scaling\"] = 100*(np.e**result_bm_width[\"slope\"] - 1)\n",
    "\n",
    "        result_bm_width = result_bm_width[[\"code\", \"Scaling\"]]\n",
    "\n",
    "    # Add to result dict \n",
    "        \n",
    "        result[\"binning_equal_width\"] = result_bm_width\n",
    "\n",
    "    df = pd.concat([df.assign(source=source) for source, df in result.items()])\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return result, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [y for y in range(2041, 2072)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:  2041\n",
      "done:  2042\n",
      "done:  2043\n",
      "done:  2044\n",
      "done:  2045\n",
      "done:  2046\n",
      "done:  2047\n",
      "done:  2048\n",
      "done:  2049\n",
      "done:  2050\n",
      "done:  2051\n",
      "done:  2052\n",
      "done:  2053\n",
      "done:  2054\n",
      "done:  2055\n",
      "done:  2056\n",
      "done:  2057\n",
      "done:  2058\n",
      "done:  2059\n",
      "done:  2060\n",
      "done:  2061\n",
      "done:  2062\n",
      "done:  2063\n",
      "done:  2064\n",
      "done:  2065\n",
      "done:  2066\n",
      "done:  2067\n",
      "done:  2068\n",
      "done:  2069\n",
      "done:  2070\n"
     ]
    }
   ],
   "source": [
    "acess585 = pd.DataFrame()\n",
    "\n",
    "for year in range(2041, 2071):\n",
    "    r,d = region_scaling(pr_dir = \"D:\\Min\\Review GCM\\ACCESS-CM2\\Interpolated\\Pr\",\n",
    "             tas_dir = \"D:\\Min\\Review GCM\\ACCESS-CM2\\Interpolated\\Tas\",\n",
    "             hur_dir = \"D:\\Min\\Review GCM\\ACCESS-CM2\\Interpolated\\Hurs\", \n",
    "             start_year =  year,\n",
    "             stop_year = year+20,\n",
    "             region = \"srex\")\n",
    "    d = d.rename(columns = {\"Scaling\": str(year+20)})\n",
    "    print(\"done: \", year)\n",
    "    if year == 2041: \n",
    "        acess585 = d \n",
    "    else: acess585 = pd.merge(acess585,d , on = [\"code\", \"source\"], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "acess585.to_csv(\"D:\\Min\\Review GCM\\ACCESS-CM2\\Interpolated\\\\result_585_srex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:  2041\n",
      "done:  2042\n",
      "done:  2043\n",
      "done:  2044\n",
      "done:  2045\n",
      "done:  2046\n",
      "done:  2047\n",
      "done:  2048\n",
      "done:  2049\n",
      "done:  2050\n",
      "done:  2051\n",
      "done:  2052\n",
      "done:  2053\n",
      "done:  2054\n",
      "done:  2055\n",
      "done:  2056\n",
      "done:  2057\n",
      "done:  2058\n",
      "done:  2059\n",
      "done:  2060\n",
      "done:  2061\n",
      "done:  2062\n",
      "done:  2063\n",
      "done:  2064\n",
      "done:  2065\n",
      "done:  2066\n",
      "done:  2067\n",
      "done:  2068\n",
      "done:  2069\n",
      "done:  2070\n"
     ]
    }
   ],
   "source": [
    "acess245 = pd.DataFrame()\n",
    "\n",
    "for year in range(2041, 2071):\n",
    "    r,d = region_scaling(pr_dir = \"D:\\Min\\Review GCM\\ACCESS-CM2\\SSP245\\Interpolated\\Pr\",\n",
    "             tas_dir = \"D:\\Min\\Review GCM\\ACCESS-CM2\\SSP245\\Interpolated\\Tas\",\n",
    "             hur_dir = \"D:\\Min\\Review GCM\\ACCESS-CM2\\SSP245\\Interpolated\\Hurs\", \n",
    "             start_year =  year,\n",
    "             stop_year = year+20,\n",
    "             region = \"srex\")\n",
    "    d = d.rename(columns = {\"Scaling\": str(year+20)})\n",
    "    print(\"done: \", year)\n",
    "    if year == 2041: \n",
    "        acess245 = d \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    else: acess245= pd.merge(acess245,d , on = [\"code\", \"source\"], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "acess245.to_csv(\"D:\\Min\\Review GCM\\ACCESS-CM2\\SSP245\\Interpolated\\\\result_245_srex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:  2041\n",
      "done:  2042\n",
      "done:  2043\n",
      "done:  2044\n",
      "done:  2045\n",
      "done:  2046\n",
      "done:  2047\n",
      "done:  2048\n",
      "done:  2049\n",
      "done:  2050\n",
      "done:  2051\n",
      "done:  2052\n",
      "done:  2053\n",
      "done:  2054\n",
      "done:  2055\n",
      "done:  2056\n",
      "done:  2057\n",
      "done:  2058\n",
      "done:  2059\n",
      "done:  2060\n",
      "done:  2061\n",
      "done:  2062\n",
      "done:  2063\n",
      "done:  2064\n",
      "done:  2065\n",
      "done:  2066\n",
      "done:  2067\n",
      "done:  2068\n",
      "done:  2069\n",
      "done:  2070\n"
     ]
    }
   ],
   "source": [
    "nor1 = pd.DataFrame()\n",
    "\n",
    "for year in range(2041, 2071):\n",
    "    r,d = region_scaling(pr_dir = \"D:\\Min\\Review GCM\\\\NorESM2-LM\\SSP245\\Interpolated\\Pr\",\n",
    "             tas_dir = \"D:\\Min\\Review GCM\\\\NorESM2-LM\\SSP245\\Interpolated\\Tas\",\n",
    "             hur_dir = \"D:\\Min\\Review GCM\\\\NorESM2-LM\\SSP245\\Interpolated\\Hurs\", \n",
    "             start_year =  year,\n",
    "             stop_year = year+20,\n",
    "             region = \"srex\")\n",
    "    d = d.rename(columns = {\"Scaling\": str(year+20)})\n",
    "    print(\"done: \", year)\n",
    "    if year == 2041: \n",
    "        nor1 = d \n",
    "    else: nor1= pd.merge(nor1,d , on = [\"code\", \"source\"], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor1.to_csv(\"D:\\Min\\Review GCM\\\\NorESM2-LM\\SSP245\\\\result_245_srex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 17-18: malformed \\N character escape (2792044683.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[47], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    tas_dir = \"D:\\Min\\Review GCM\\NorESM2-LM\\\\SSP585\\Interpolated\\Tas\",\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 17-18: malformed \\N character escape\n"
     ]
    }
   ],
   "source": [
    "nor2 = pd.DataFrame()\n",
    "\n",
    "for year in range(2041, 2071):\n",
    "    r,d = region_scaling(pr_dir = \"D:\\Min\\Review GCM\\\\NorESM2-LM\\SSP585\\Interpolated\\Pr\",\n",
    "             tas_dir = \"D:\\Min\\Review GCM\\NorESM2-LM\\\\SSP585\\Interpolated\\Tas\",\n",
    "             hur_dir = \"D:\\Min\\Review GCM\\NorESM2-LM\\\\SSP585\\Interpolated\\Hurs\", \n",
    "             start_year =  year,\n",
    "             stop_year = year+20,\n",
    "             region = \"srex\")\n",
    "    d = d.rename(columns = {\"Scaling\": str(year+20)})\n",
    "    print(\"done: \", year)\n",
    "    if year == 2041: \n",
    "        nor2 = d \n",
    "    else: nor2 = pd.merge(nor2,d , on = [\"code\", \"source\"], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor2.to_csv(\"D:\\Min\\Review GCM\\\\NorESM2-LM\\SSP585\\\\result_585_srex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
